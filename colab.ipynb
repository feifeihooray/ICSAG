{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-01T01:58:09.434364800Z",
     "start_time": "2023-08-01T01:58:07.379617900Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SWaT_Dataset_Attack_v0.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 49\u001B[0m\n\u001B[0;32m     42\u001B[0m     test_loader \u001B[38;5;241m=\u001B[39m DataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m X_train, X_test, y_train, y_test, train_loader, test_loader\n\u001B[1;32m---> 49\u001B[0m X_train, X_test, y_train, y_test, train_loader, test_loader \u001B[38;5;241m=\u001B[39m \u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 11\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m(batch_size)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data\u001B[39m(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# data = pd.read_excel('SWaT_Dataset_Attack_v0.xlsx')\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSWaT_Dataset_Attack_v0.xlsx\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTimestamp\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     14\u001B[0m     sequence_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pt1\\lib\\site-packages\\pandas\\util\\_decorators.py:299\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    294\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    295\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting with Pandas version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m all arguments of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    296\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00marguments\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m will be keyword-only\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    297\u001B[0m     )\n\u001B[0;32m    298\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(msg, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39mstacklevel)\n\u001B[1;32m--> 299\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pt1\\lib\\site-packages\\pandas\\io\\excel\\_base.py:336\u001B[0m, in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[0;32m    335\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 336\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    339\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    340\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    341\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pt1\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1071\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[0;32m   1069\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxls\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1071\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1072\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[0;32m   1073\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1075\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mods\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1076\u001B[0m     engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124modf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pt1\\lib\\site-packages\\pandas\\io\\excel\\_base.py:949\u001B[0m, in \u001B[0;36minspect_excel_format\u001B[1;34m(path, content, storage_options)\u001B[0m\n\u001B[0;32m    946\u001B[0m     content_or_path \u001B[38;5;241m=\u001B[39m content \u001B[38;5;129;01mor\u001B[39;00m path\n\u001B[0;32m    947\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m content_or_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 949\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    950\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m    951\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[0;32m    952\u001B[0m     stream \u001B[38;5;241m=\u001B[39m handle\u001B[38;5;241m.\u001B[39mhandle\n\u001B[0;32m    953\u001B[0m     stream\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pt1\\lib\\site-packages\\pandas\\io\\common.py:651\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    642\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    643\u001B[0m             handle,\n\u001B[0;32m    644\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    647\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    648\u001B[0m         )\n\u001B[0;32m    649\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    650\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 651\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    652\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    654\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'SWaT_Dataset_Attack_v0.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "def load_data(batch_size=32):\n",
    "    # data = pd.read_excel('/content/drive/MyDrive/SWaT/SWaT.A1 & A2_Dec 2015/Physical/SWaT_Dataset_Attack_v0.xlsx')\n",
    "    data = pd.read_excel('SWaT_Dataset_Attack_v0.xlsx')\n",
    "    data = data.drop(columns=['Timestamp'])\n",
    "\n",
    "    sequence_length = 10\n",
    "    temp_data = []\n",
    "\n",
    "    for column in data.columns[:-1]:\n",
    "        for i in range(sequence_length):\n",
    "            shifted = data[column].shift(i)\n",
    "            temp_data.append(shifted.rename(column+'_'+str(i)))\n",
    "\n",
    "    new_data = pd.concat(temp_data, axis=1).dropna()\n",
    "    X = new_data.values\n",
    "    X = X.reshape((X.shape[0], sequence_length, -1))\n",
    "\n",
    "    Y = data[\"Normal/Attack\"]\n",
    "    encoder = LabelEncoder()\n",
    "    Y = encoder.fit_transform(Y)\n",
    "    Y = Y[new_data.index]\n",
    "\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = X.reshape(-1, sequence_length, X.shape[-1])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).float().unsqueeze(1))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float().unsqueeze(1))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_loader, test_loader = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import BCELoss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(conf_mat, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = conf_mat.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n",
    "        plt.text(j, i, format(conf_mat[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_mat[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_dim, 100, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(100, 100, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm3 = nn.LSTM(100, 50, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    # X_train, X_test, y_train, y_test, train_loader, test_loader = load_data()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = LSTMModel(train_loader.dataset.tensors[0].shape[2]).to(device)\n",
    "\n",
    "    criterion = BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    for epoch in tqdm(range(20), desc=\"Training Progress\"):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                all_labels = []\n",
    "                all_predictions = []\n",
    "\n",
    "                for i, data in enumerate(test_loader):\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    predicted = torch.round(outputs)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    # collect all labels and predictions for confusion matrix\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "                accuracy = correct / total\n",
    "                conf_mat = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "            print(f'Epoch {epoch}, loss: {running_loss/len(train_loader)}, val_loss: {val_loss/len(test_loader)}, accuracy: {accuracy}')\n",
    "            print(f'Confusion Matrix: \\n {conf_mat}')\n",
    "\n",
    "            # Plot confusion matrix\n",
    "            classes = ['Class 0', 'Class 1'] # replace with your class names if different\n",
    "            plt.figure()\n",
    "            plot_confusion_matrix(conf_mat, classes=classes)\n",
    "            plt.savefig('confusion_matrix.png')\n",
    "\n",
    "        # Save the model\n",
    "        if not os.path.exists('model'):\n",
    "            os.makedirs('model')\n",
    "        torch.save(model.state_dict(), 'model/model.ckpt')\n",
    "        print('Model saved to model/model.ckpt')\n",
    "\n",
    "\n",
    "train_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
